# -*- coding: utf-8 -*-
"""MV-HSI-CNN_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s5iJP05L5vPCJE4lGD8Zk30BQLgWoY2u
"""

from google.colab import drive
drive.mount('/content/drive')

!rm ./ata/ -r
!unzip "/content/drive/My Drive/DATASET_85K.zip" -d ./ata

!pip install -q keras
import keras
import tensorflow as tf
import PIL.Image as Image
from PIL import ImageFilter
import numpy as np
import IPython.display as display
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import pathlib
import random
import math
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
from sklearn.model_selection import train_test_split
import cv2
import seaborn as sns
from math import ceil,floor
from keras.preprocessing.image import ImageDataGenerator
!pip install split-folders
import splitfolders

from keras.applications import vgg16, inception_v3, resnet50, mobilenet
from keras.preprocessing.image import img_to_array

model=tf.keras.applications.InceptionV3(
    include_top=True, weights=None, input_tensor=None, input_shape=(75,75,3),
    pooling=None, classes=1000, classifier_activation='softmax'
)
tf.keras.utils.plot_model(model,show_layer_names=True,show_shapes=True)
model.summary()
optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam');
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model=tf.keras.applications.VGG16(
    include_top=True, weights=None, input_tensor=None, input_shape=(36,44,3),
    pooling=None, classes=1000, classifier_activation='softmax'
)

tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=True)
model.summary()
optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam');
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model=tf.keras.applications.ResNet50(
    include_top=True, weights=None, input_tensor=None, input_shape=(36,44,3),
    pooling=None, classes=1000, classifier_activation='softmax')
tf.keras.utils.plot_model(model,show_layer_names=True,show_shapes=True)
model.summary()
optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam');
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

BATCH_SIZE = 32
IMG_HEIGHT = 352
IMG_WIDTH = 288
MB_SIZE = 8

splitfolders.ratio("ata", output="data", seed=1337, ratio=(.8, .1, .1), group_prefix=None) # default values

datagen = ImageDataGenerator(rescale=1./255)

train_it = datagen.flow_from_directory('data/train/',  batch_size=32,color_mode="rgb",target_size=(36,44),class_mode='sparse')
# load and iterate validation dataset
val_it = datagen.flow_from_directory('data/val/',  batch_size=32,target_size=(36,44),class_mode='sparse')
# load and iterate test dataset
test_it = datagen.flow_from_directory('data/test/', batch_size=32,target_size=(36,44),class_mode='sparse')

STEP_SIZE_TRAIN=train_it.n//train_it.batch_size
STEP_SIZE_VALID=val_it.n//val_it.batch_size

print(STEP_SIZE_TRAIN)

history = model.fit_generator(train_it,
                    steps_per_epoch=STEP_SIZE_TRAIN,
                    validation_data=val_it,
                    validation_steps=STEP_SIZE_VALID,
                    epochs=10
)

STEP_SIZE_TEST=test_it.n//test_it.batch_size

model.evaluate(test_it,
steps=STEP_SIZE_TEST)
# print(steps)

SEED = 44000


# lists to store data
data = []
label = []

BASE_FOLDER = 'ata/'

folders = os.listdir(BASE_FOLDER)
folders=['still','ZOOM_IN','ZOOM_OUT','PAN_RIGHT','PAN_LEFT','TILT_UP','TILT_DOWN','TILT_UP_PAN_RIGHT','TILT_UP_PAN_LEFT','TILT_DOWN_PAN_LEFT','TILT_DOWN_PAN_RIGHT']
folder_labels = ['SC','ZIC','ZOC','PRC','PLC','TUC','TDC','TUPRC','TUPLC','TDPLC','TDPRC']

CLASS_NAMES = folders;
data_labels = {};

for i in range(len(folders)):
  data_labels[folders[i]] = i


print(len( folders))
# loading data to lists
for folder in folders:
  for file in os.listdir(BASE_FOLDER + folder + '/'):
    # print(BASE_FOLDER + folder + '/' + file)
    img = Image.open(BASE_FOLDER + folder + '/' + file)
    img = img.convert('RGB')
    img = img.filter(ImageFilter.MedianFilter(3));
    # print(np.shape(img))
    img=np.reshape(img,(36,44,3))
    img=cv2.resize(img,(75,75), interpolation = cv2.INTER_AREA)
    # do any pre-processing if needed like resize, sharpen etc.
    # img = cv2.medianBlur(img,3);
    # print(np.shape(img))
    data.append(np.asarray(img)/255.0)

    label.append(data_labels[folder])
# now split the data in to train and test with the help of train_test_split
train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.3, random_state=SEED)
del data
del label

BATCH_SIZE = 32
IMG_HEIGHT = 352
IMG_WIDTH = 288
MB_SIZE = 8

# model = inception_v3.InceptionV3(weights='imagenet')
# model = vgg16.VGG16(weights='imagenet')
# model=tf.keras.applications.InceptionV3(
#     include_top=True, weights='imagenet', input_tensor=None, input_shape=None,
#     pooling=None, classes=1000, classifier_activation='softmax'
# )

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32,kernel_size=3,strides=(1,1),padding='same',activation='relu',input_shape=(int(IMG_WIDTH/MB_SIZE),int(IMG_HEIGHT/MB_SIZE),3)),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),
  # tf.keras.layers.BatchNormalization(),
 
  tf.keras.layers.Conv2D(64,kernel_size=3,padding='same',activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),

  tf.keras.layers.Conv2D(64,kernel_size=3,padding="same",activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),

  tf.keras.layers.Conv2D(128,kernel_size=3,padding="same",activation='relu'),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),
  
  tf.keras.layers.Conv2D(256,kernel_size=3,padding="same",activation='relu'),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),
  tf.keras.layers.Flatten(),
  
  tf.keras.layers.Dense(1024, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(512, activation='relu'),
  # tf.keras.layers.Dense(512, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.1),
  tf.keras.layers.Dense(256, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.2),
  # tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')
  tf.keras.layers.Dense(11, activation='softmax')
])

optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam');
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
tf.keras.utils.plot_model(model,show_shapes=True)
model.summary()
# processed_image = vgg16.preprocess_input(image_batch.copy())
 
# # get the predicted probabilities for each class
# predictions = vgg_model.predict(processed_image)

history = model.fit(np.array(train_data), np.array(train_label), epochs=1,validation_split=.1)

# print(np.shape(test_data))

model.evaluate(np.array(test_data),np.array(test_label))

# print(history.history.keys())
# fig=plt.figure()
# for i in history.history.keys():
#   print(i)
#   plt.plot(history.history[i])
#   plt.title(i)
# plt.legend(history.history.keys())
# plt.show()
# fig.savefig('fig1.png',dpi=1000)


# fig=plt.figure()
# plt.plot(history.history['accuracy'])
# plt.plot(history.history['val_accuracy'])
# plt.title('model accuracy')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()
# fig.savefig('fig2.png',dpi=1000)



# # summarize history for loss
# fig=plt.figure()
# plt.plot(history.history['loss'])
# plt.plot(history.history['val_loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'test'], loc='upper left')
# plt.show()
# fig.savefig('fig3.png',dpi=1000)

# Y_P = model.predict_classes(np.array(test_data))
# # print(Y_P)
# M = confusion_matrix(np.array(test_label),Y_P)
# print(M)
# M = confusion_matrix(np.array(test_label),Y_P,normalize='true')
# M = np.around(M,decimals=2)
# # print(M)
# print(classification_report(np.array(test_label),Y_P))
# for i in range(0,len(folders)):
#   print(i,folders[i])
# # print(test_data[1])

# fig, ax = plt.subplots(figsize=(10,10))
# sns.set(font_scale=1.3)
# sns.set_context(font_scale=2 )
# sns.axes_style()
# g=sns.heatmap(M, cmap="YlGnBu",annot=True, fmt='.2f', xticklabels=folder_labels, yticklabels=folder_labels,cbar=False)
# plt.yticks(rotation=0)

# plt.ylabel('Actual')
# plt.xlabel('Predicted')
# plt.show(block=False)
# fig.savefig('confusion_matrix.png',dpi=1000, )

# accuracy_score(np.array(test_label),Y_P)