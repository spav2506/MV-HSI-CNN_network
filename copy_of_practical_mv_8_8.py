# -*- coding: utf-8 -*-
"""Copy_of_practical_mv_8_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sxoQF6747lgUYaMQhpdaArNRsA2Bjv8I
"""

import tensorflow as tf
import PIL.Image as Image
from PIL import ImageFilter
import numpy as np
import IPython.display as display
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import pathlib
import random
import math
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
from sklearn.model_selection import train_test_split
import cv2
import csv
import pandas as pd

import seaborn as sns
from math import ceil,floor

from google.colab import drive
drive.mount('/content/drive')
!rm ata -r
!rm test -r
# !cp drive/My\ Drive/data_8_8.zip ./
# !unzip data_8_8.zip -data

# !cp drive/My\ Drive/DATASET_ESME_NEW.zip ./
# # !unzip -q DATASET_ESME_NEW.zip -d test 
# !cp drive/My\ Drive/data_new_simulation.zip ./
# !unzip -q data_new_simulation.zip -d ata

# !cp drive/My\ Drive/CAMHID.zip ./
# !unzip -q CAMHID.zip -d ata


# !cp drive/My\ Drive/Data.zip ./
# !unzip Data.zip -data

!cp drive/My\ Drive/DATASET_CSV.zip ./
!unzip -q DATASET_CSV.zip -data

# d = np.fromfile('test.csv',sep=',')
# d = pd.read_csv('test.csv',header=None)

# print(np.asanyarray(d))
random.random()
np.random.permutation(10)

SEED = 44000


# lists to store data
data = []
label = []

# folder where data is placed
BASE_FOLDER = 'ata/'
CLASS_NAMES =  os.listdir(BASE_FOLDER);
folders = os.listdir(BASE_FOLDER)
data_labels = {};

for i in range(len(folders)):
  data_labels[folders[i]] = i


print(len( folders))
# loading data to lists
for folder in folders:
  print('reading',folder)
  C=0
  files = os.listdir(BASE_FOLDER + folder + '/')
  np.random.shuffle(files)
  # print(files)
  for file in range(min(5000,len(files))):
    # print(BASE_FOLDER + folder + '/' + file)
    C=C+1;
    if(C>100):
      pass
      # break;
    img = pd.read_csv(BASE_FOLDER + folder + '/' + files[file],header=None)
    d = np.asarray(img)
    # d = np.reshape(d,(72,44,1))
    c = np.zeros((36,44,2))
    c[:,:,0] = d[0:36,:];
    c[:,:,1] = d[36:,:];
    # print(np.shape(d))
    
    data.append(np.asarray(c))

    label.append(data_labels[folder])
# now split the data in to train and test with the help of train_test_split

train_data, test_data, train_label, test_label = train_test_split(data, label, test_size=0.2, random_state=SEED)
del data
del data_labels

print(np.shape(train_data))
print(np.shape(c))
print(train_label)
print(np.shape(train_label))
print(np.shape(label))

BATCH_SIZE = 32
IMG_HEIGHT = 352
IMG_WIDTH = 288
MB_SIZE = 8

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32,kernel_size=3,strides=(1,1),padding='same',activation='relu',input_shape=(int(IMG_WIDTH/MB_SIZE),int(IMG_HEIGHT/MB_SIZE),2)),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),
  # tf.keras.layers.BatchNormalization(),
 
  tf.keras.layers.Conv2D(64,kernel_size=3,padding='same',activation='relu'),
  # tf.keras.layers.BatchNormalization(),0
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),

  tf.keras.layers.Conv2D(64,kernel_size=3,padding="same",activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),

  tf.keras.layers.Conv2D(128,kernel_size=3,padding="same",activation='relu'),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),
  
  tf.keras.layers.Conv2D(256,kernel_size=3,padding="same",activation='relu'),
  tf.keras.layers.MaxPool2D(pool_size=(2,2)),



  tf.keras.layers.Flatten(),
  
  tf.keras.layers.Dense(1024, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(512, activation='relu'),
  # tf.keras.layers.Dense(512, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.1),
  tf.keras.layers.Dense(256, activation='relu'),
  # tf.keras.layers.BatchNormalization(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')
])
optimizer = tf.keras.optimizers.Adam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam');
model.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
tf.keras.utils.plot_model(model,show_shapes=True)
# model.summary()

print(np.shape(train_data))
print(np.shape(train_label))


history = model.fit(np.array(train_data), np.array(train_label), epochs=1,validation_split=.1)

print(np.shape(test_data))

model.evaluate(np.array(test_data),np.array(test_label))

# Y_P = model.predict_classes(np.array(test_data))
# # print(Y_P)
# M = confusion_matrix(np.array(test_label),Y_P)
# print(M)
# M = confusion_matrix(np.array(test_label),Y_P,normalize='true')
# M = np.around(M,decimals=2)
# # print(M)
# print(classification_report(np.array(test_label),Y_P))
# for i in range(0,len(folders)):
#   print(i,folders[i])
# # print(test_data[1])

# fig, ax = plt.subplots(figsize=(10,10))
# sns.set(font_scale=1.3)
# sns.set_context(font_scale=1.3)
# sns.axes_style()
# g=sns.heatmap(M, cmap="YlGnBu",annot=True, fmt='.2f', xticklabels=folders, yticklabels=folders,cbar=False)
# plt.yticks(rotation=0)
# plt.ylabel('Actual')
# plt.xlabel('Predicted')
# plt.show(block=False)
# fig.savefig('test.png',dpi=1000, )

# accuracy_score(np.array(test_label),Y_P)